{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util, nltk.metrics\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.metrics import *\n",
    "from nltk.metrics.scores import accuracy, precision, recall, f_measure\n",
    "                                     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (\"C:\\\\Users\\\\Myles\\\\Documents\\\\Napier Data Science\\\\Wranging Assment\\\\Submission\\\\training.txt\")\n",
    "df = pd.read_table(path,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the colums to something more meaningfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['sentiment', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to format and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_feats(sentence):\n",
    "    sentence = sentence.lower() \n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return dict([(word, True) for word in words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a very simple dictionary which maps a feature name (word) to True if the word exists in the data. Test the function on sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blab': True, 'sentenc': True, 'swim': True, 'test': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_word_feats(\"blab and if swimming Test sentence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a generative classifier, i.e. it builds a model of each class and given an observation, it returns the class most likely to have generated the observation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = df.loc[df['sentiment'] == 0]\n",
    "pos = df.loc[df['sentiment'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier training method expects to be given a list of tokens in the form of [(feats, label)] where feats is a feature dictionary and label is the classification label. In our case, feats will be of the form {word: True} and label will be one of ‘pos’ or ‘neg’.Creating feature-label pairs where the features will be a feature dictionary in the form of {word: True} and the label is either a \"pos\" or a \"neg\" label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negreviews = [(extract_word_feats(f), 'neg') for f in neg['text']]\n",
    "posreviews = [(extract_word_feats(f), 'pos') for f in pos['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the reviews into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = negreviews + posreviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and test with a 80%training 20% test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5534.400000000001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)* 0.8 # multipling the length by 0.8 to get 80% of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = data[:5534]\n",
    "test = data[5535:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Naive Bayes classifier and create classifer variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9768618944323934"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.util.accuracy(classifier, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-proscessing for alternative metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (feats, label) in enumerate(test):\n",
    "    refsets[label].add(i)\n",
    "    predicted = classifier.classify(feats)\n",
    "    testsets[predicted].add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying alternate accuracy measures within the NLTK package Precision, recall and the combined F-measure score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision: 0.9744245524296675\n",
      "pos recall: 0.9844961240310077\n",
      "pos F-measure: 0.9794344473007711\n",
      "neg precision: 0.9800332778702163\n",
      "neg recall: 0.9671592775041051\n",
      "neg F-measure: 0.9735537190082646\n"
     ]
    }
   ],
   "source": [
    "print('pos precision:', nltk.precision(refsets['pos'], testsets['pos']))\n",
    "print('pos recall:', nltk.recall(refsets['pos'], testsets['pos']))\n",
    "print('pos F-measure:', nltk.f_measure(refsets['pos'], testsets['pos']))\n",
    "print('neg precision:', nltk.precision(refsets['neg'], testsets['neg']))\n",
    "print('neg recall:', nltk.recall(refsets['neg'], testsets['neg']))\n",
    "print('neg F-measure:', nltk.f_measure(refsets['neg'], testsets['neg']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
